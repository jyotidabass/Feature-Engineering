{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wZJ26znQ4drHVNOgHg9l96Lx3I8cA_lt",
      "authorship_tag": "ABX9TyNfeXkg6DapeC/sJf3Vg/kj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Feature-Engineering/blob/main/Traditional%20texture%20features\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-1SBCAuxb4Y",
        "outputId": "a5fbddb0-8447-4f0f-9817-466eab2282e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Traditional-Feature-Extraction'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/Rohit-Kundu/Traditional-Feature-Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 texture features are extracted from this\n",
        "!pip install mahotas\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import mahotas as mt\n",
        "from sklearn.svm import LinearSVC\n",
        "import csv\n",
        "import time\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "# function to extract haralick textures from an image\n",
        "def extract_features(image):\n",
        "    # calculate haralick texture features for 4 types of adjacency\n",
        "    textures = mt.features.haralick(image)\n",
        "\n",
        "    # take the mean of it and return it\n",
        "    ht_mean  = textures.mean(axis=0)\n",
        "    return ht_mean\n",
        "\n",
        "# load the training dataset\n",
        "train_path  = '/content/sample_data' #Enter the directory where all the images are stored\n",
        "train_names = os.listdir(train_path)\n",
        "\n",
        "\n",
        "# empty list to hold feature vectors and train labels\n",
        "train_features = []\n",
        "train_labels   = []\n",
        "\n",
        "# loop over the training dataset\n",
        "print (\"[STATUS] Started extracting haralick textures..\")\n",
        "cur_path = os.path.join(train_path, '*g')\n",
        "cur_label = train_names\n",
        "i = 0\n",
        "with open('Haralick_BreaKHis_temp.csv','a+',newline='') as obj:\n",
        "                writer = csv.writer(obj)\n",
        "                if i==0:\n",
        "                        writer.writerow(['Haralick1','Haralick2','Haralick3','Haralick4','Haralick5','Haralick6','Haralick7','Haralick8','Haralick9',\n",
        "                                         'Haralick10','Haralick11','Haralick12','Haralick13'])\n",
        "                for file in glob.glob(cur_path):\n",
        "                    print (\"Processing Image - {} in {}\".format(i, cur_label[i]))\n",
        "                    #read the training image\n",
        "                    image=cv2.imread(file)\n",
        "\n",
        "                    #convert the image to grayscale\n",
        "                    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "                    \n",
        "                    #extract haralick texture from image\n",
        "                    features=extract_features(gray)\n",
        "                    #print(features)\n",
        "                    \n",
        "                    #append the feature vector and label\n",
        "                    train_features.append(features)\n",
        "                    train_labels.append(cur_label[i])\n",
        "\n",
        "                    \n",
        "                    writer.writerow(features)\n",
        "\n",
        "                    #show loop update\n",
        "                    i+=1\n",
        "\n",
        "    \n",
        "# have a look at the size of our feature vector and labels\n",
        "print (\"Training features: {}\".format(np.array(train_features).shape))\n",
        "print (\"Training labels: {}\".format(np.array(train_labels).shape))\n",
        "\n",
        "toc = time.time()\n",
        "print(\"Computation time is {} minutes.\".format((toc-tic)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMljgb4xxkcQ",
        "outputId": "225f02fd-6c4f-4697-ce34-72b8e389692c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "[STATUS] Started extracting haralick textures..\n",
            "Training features: (0,)\n",
            "Training labels: (0,)\n",
            "Computation time is 4.6133995056152344e-05 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TAMURA FEATURES"
      ],
      "metadata": {
        "id": "ZcrQkHzu19AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4 texture features are extracted from this\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "def coarseness(image, kmax):\n",
        "\timage = np.array(image)\n",
        "\tw = image.shape[0]\n",
        "\th = image.shape[1]\n",
        "\tkmax = kmax if (np.power(2,kmax) < w) else int(np.log(w) / np.log(2))\n",
        "\tkmax = kmax if (np.power(2,kmax) < h) else int(np.log(h) / np.log(2))\n",
        "\taverage_gray = np.zeros([kmax,w,h])\n",
        "\thorizon = np.zeros([kmax,w,h])\n",
        "\tvertical = np.zeros([kmax,w,h])\n",
        "\tSbest = np.zeros([w,h])\n",
        "\n",
        "\tfor k in range(kmax):\n",
        "\t\twindow = np.power(2,k)\n",
        "\t\tfor wi in range(w)[window:(w-window)]:\n",
        "\t\t\tfor hi in range(h)[window:(h-window)]:\n",
        "\t\t\t\taverage_gray[k][wi][hi] = np.sum(image[wi-window:wi+window, hi-window:hi+window])\n",
        "\t\tfor wi in range(w)[window:(w-window-1)]:\n",
        "\t\t\tfor hi in range(h)[window:(h-window-1)]:\n",
        "\t\t\t\thorizon[k][wi][hi] = average_gray[k][wi+window][hi] - average_gray[k][wi-window][hi]\n",
        "\t\t\t\tvertical[k][wi][hi] = average_gray[k][wi][hi+window] - average_gray[k][wi][hi-window]\n",
        "\t\thorizon[k] = horizon[k] * (1.0 / np.power(2, 2*(k+1)))\n",
        "\t\tvertical[k] = horizon[k] * (1.0 / np.power(2, 2*(k+1)))\n",
        "\n",
        "\tfor wi in range(w):\n",
        "\t\tfor hi in range(h):\n",
        "\t\t\th_max = np.max(horizon[:,wi,hi])\n",
        "\t\t\th_max_index = np.argmax(horizon[:,wi,hi])\n",
        "\t\t\tv_max = np.max(vertical[:,wi,hi])\n",
        "\t\t\tv_max_index = np.argmax(vertical[:,wi,hi])\n",
        "\t\t\tindex = h_max_index if (h_max > v_max) else v_max_index\n",
        "\t\t\tSbest[wi][hi] = np.power(2,index)\n",
        "\n",
        "\tfcrs = np.mean(Sbest)\n",
        "\treturn fcrs\n",
        "\n",
        "\n",
        "def contrast(image):\n",
        "\timage = np.array(image)\n",
        "\timage = np.reshape(image, (1, image.shape[0]*image.shape[1]))\n",
        "\tm4 = np.mean(np.power(image - np.mean(image),4))\n",
        "\tv = np.var(image)\n",
        "\tstd = np.power(v, 0.5)\n",
        "\talfa4 = m4 / np.power(v,2)\n",
        "\tfcon = std / np.power(alfa4, 0.25)\n",
        "\treturn fcon\n",
        "\n",
        "def directionality(image):\n",
        "\timage = np.array(image, dtype = 'int64')\n",
        "\th = image.shape[0]\n",
        "\tw = image.shape[1]\n",
        "\tconvH = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])\n",
        "\tconvV = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\n",
        "\tdeltaH = np.zeros([h,w])\n",
        "\tdeltaV = np.zeros([h,w])\n",
        "\ttheta = np.zeros([h,w])\n",
        "\n",
        "\t# calc for deltaH\n",
        "\tfor hi in range(h)[1:h-1]:\n",
        "\t\tfor wi in range(w)[1:w-1]:\n",
        "\t\t\tdeltaH[hi][wi] = np.sum(np.multiply(image[hi-1:hi+2, wi-1:wi+2], convH))\n",
        "\tfor wi in range(w)[1:w-1]:\n",
        "\t\tdeltaH[0][wi] = image[0][wi+1] - image[0][wi]\n",
        "\t\tdeltaH[h-1][wi] = image[h-1][wi+1] - image[h-1][wi]\n",
        "\tfor hi in range(h):\n",
        "\t\tdeltaH[hi][0] = image[hi][1] - image[hi][0]\n",
        "\t\tdeltaH[hi][w-1] = image[hi][w-1] - image[hi][w-2]\n",
        "\n",
        "\t# calc for deltaV\n",
        "\tfor hi in range(h)[1:h-1]:\n",
        "\t\tfor wi in range(w)[1:w-1]:\n",
        "\t\t\tdeltaV[hi][wi] = np.sum(np.multiply(image[hi-1:hi+2, wi-1:wi+2], convV))\n",
        "\tfor wi in range(w):\n",
        "\t\tdeltaV[0][wi] = image[1][wi] - image[0][wi]\n",
        "\t\tdeltaV[h-1][wi] = image[h-1][wi] - image[h-2][wi]\n",
        "\tfor hi in range(h)[1:h-1]:\n",
        "\t\tdeltaV[hi][0] = image[hi+1][0] - image[hi][0]\n",
        "\t\tdeltaV[hi][w-1] = image[hi+1][w-1] - image[hi][w-1]\n",
        "\n",
        "\tdeltaG = (np.absolute(deltaH) + np.absolute(deltaV)) / 2.0\n",
        "\tdeltaG_vec = np.reshape(deltaG, (deltaG.shape[0] * deltaG.shape[1]))\n",
        "\n",
        "\t# calc the theta\n",
        "\tfor hi in range(h):\n",
        "\t\tfor wi in range(w):\n",
        "\t\t\tif (deltaH[hi][wi] == 0 and deltaV[hi][wi] == 0):\n",
        "\t\t\t\ttheta[hi][wi] = 0;\n",
        "\t\t\telif(deltaH[hi][wi] == 0):\n",
        "\t\t\t\ttheta[hi][wi] = np.pi\n",
        "\t\t\telse:\n",
        "\t\t\t\ttheta[hi][wi] = np.arctan(deltaV[hi][wi] / deltaH[hi][wi]) + np.pi / 2.0\n",
        "\ttheta_vec = np.reshape(theta, (theta.shape[0] * theta.shape[1]))\n",
        "\n",
        "\tn = 16\n",
        "\tt = 12\n",
        "\tcnt = 0\n",
        "\thd = np.zeros(n)\n",
        "\tdlen = deltaG_vec.shape[0]\n",
        "\tfor ni in range(n):\n",
        "\t\tfor k in range(dlen):\n",
        "\t\t\tif((deltaG_vec[k] >= t) and (theta_vec[k] >= (2*ni-1) * np.pi / (2 * n)) and (theta_vec[k] < (2*ni+1) * np.pi / (2 * n))):\n",
        "\t\t\t\thd[ni] += 1\n",
        "\thd = hd / np.mean(hd)\n",
        "\thd_max_index = np.argmax(hd)\n",
        "\tfdir = 0\n",
        "\tfor ni in range(n):\n",
        "\t\tfdir += np.power((ni - hd_max_index), 2) * hd[ni]\n",
        "\treturn fdir\n",
        "\n",
        "def roughness(fcrs, fcon):\n",
        "\treturn fcrs + fcon\n",
        "\n",
        "if __name__ == '__main__':\n",
        "        \n",
        "        # load the training dataset\n",
        "        train_path  = '/content/sample_data'\n",
        "        train_names = os.listdir(train_path)\n",
        "\n",
        "        # loop over the training dataset\n",
        "        cur_path = os.path.join(train_path, '*g')\n",
        "        cur_label = train_names\n",
        "        i = 0\n",
        "\n",
        "        for file in glob.glob(cur_path):\n",
        "            print('For image {} named {}:'.format(i+1,cur_label[i]))\n",
        "            img = cv2.imread(file)\n",
        "            img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "            print('Shape of image is: {} '.format(img.shape))\n",
        "            fcrs = coarseness(img, 5)\n",
        "            print(\"coarseness: %f\" % fcrs);\n",
        "            fcon = contrast(img)\n",
        "            print(\"contrast: %f\" % fcon)\n",
        "            fdir= directionality(img)\n",
        "            print(\"directionality: %f\" % fdir)\n",
        "            f_r=roughness(fcrs,fcon)\n",
        "            print(\"roughness: %f\" % f_r)\n",
        "            print('\\n\\n')\n",
        "            i+=1\n",
        "\n",
        "toc = time.time()\n",
        "print(\"Computation time is {} minutes\".format((toc-tic)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGaoKbfp2Bla",
        "outputId": "2afc7643-7df2-48d8-a0bb-7398ad8ab5f3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computation time is 2.901156743367513e-05 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAY LEVEL RUN LENGTH MATRIX"
      ],
      "metadata": {
        "id": "IdTK_PHW2V0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GLRLM or Gray Level Run Length Matrix\n",
        "#33 different features are extracted from this\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image \n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "\n",
        "data = 0 \n",
        "def read_img(path=\" \"):\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(path) \n",
        "            img = img.convert('L')\n",
        "            self.data=np.array(img)\n",
        "            \n",
        "        except:\n",
        "            img = None\n",
        "            \n",
        "def getGrayLevelRumatrix(array, theta):\n",
        "\n",
        "            \n",
        "            #array: the numpy array of the image\n",
        "            #theta: Input, the angle used when calculating the gray scale run matrix, list type, can contain fields:['deg0', 'deg45', 'deg90', 'deg135']\n",
        "            #glrlm: output,the glrlm result\n",
        "\n",
        "            P = array\n",
        "            x, y = P.shape\n",
        "            min_pixels = np.min(P)   # the min pixel\n",
        "            run_length = max(x, y)   # Maximum parade length in pixels\n",
        "            num_level = np.max(P) - np.min(P) + 1   # Image gray level\n",
        "    \n",
        "            deg0 = [val.tolist() for sublist in np.vsplit(P, x) for val in sublist]   # 0deg\n",
        "            deg90 = [val.tolist() for sublist in np.split(np.transpose(P), y) for val in sublist]   # 90deg\n",
        "            diags = [P[::-1, :].diagonal(i) for i in range(-P.shape[0]+1, P.shape[1])]   #45deg\n",
        "            deg45 = [n.tolist() for n in diags]\n",
        "            Pt = np.rot90(P, 3)   # 135deg\n",
        "            diags = [Pt[::-1, :].diagonal(i) for i in range(-Pt.shape[0]+1, Pt.shape[1])]\n",
        "            deg135 = [n.tolist() for n in diags]\n",
        "    \n",
        "            def length(l):\n",
        "                if hasattr(l, '_len_'):\n",
        "                    return np.size(l)\n",
        "                else:\n",
        "                    i = 0\n",
        "                    for _ in l:\n",
        "                        i += 1\n",
        "                    return i\n",
        "    \n",
        "            glrlm = np.zeros((num_level, run_length, len(theta)))   \n",
        "            for angle in theta:\n",
        "                for splitvec in range(0, len(eval(angle))):\n",
        "                    flattened = eval(angle)[splitvec]\n",
        "                    answer = []\n",
        "                    for key, iter in groupby(flattened):  \n",
        "                        answer.append((key, length(iter)))   \n",
        "                    for ansIndex in range(0, len(answer)):\n",
        "                        glrlm[int(answer[ansIndex][0]-min_pixels), int(answer[ansIndex][1]-1), theta.index(angle)] += 1   \n",
        "            return glrlm\n",
        "\n",
        " # The gray scale run matrix is only the measurement and statistics of the image pixel information. In the actual use process, the generated\n",
        "  #   The gray scale run matrix is calculated to obtain image feature information based on the gray level co-occurrence matrix.\n",
        "  #   First write a few common functions to complete the calculation of subscripts i and j (calcuteIJ ()), multiply and divide according to the specified dimension (apply_over_degree ())\n",
        "   #  And calculate the sum of all pixels (calcuteS ())\n",
        "        \n",
        "            \n",
        "def apply_over_degree(function, x1, x2):\n",
        "        rows, cols, nums = x1.shape\n",
        "        result = np.ndarray((rows, cols, nums))\n",
        "        for i in range(nums):\n",
        "                #print(x1[:, :, i])\n",
        "                result[:, :, i] = function(x1[:, :, i], x2)\n",
        "               # print(result[:, :, i])\n",
        "                result[result == np.inf] = 0\n",
        "                result[np.isnan(result)] = 0\n",
        "        return result \n",
        "def calcuteIJ (rlmatrix):\n",
        "        gray_level, run_length, _ = rlmatrix.shape\n",
        "        I, J = np.ogrid[0:gray_level, 0:run_length]\n",
        "        return I, J+1\n",
        "\n",
        "def calcuteS(rlmatrix):\n",
        "        return np.apply_over_axes(np.sum, rlmatrix, axes=(0, 1))[0, 0]\n",
        "    \n",
        "    #The following code realizes the extraction of 11 gray runoff matrix features\n",
        "   \n",
        "    #1.SRE\n",
        "def getShortRunEmphasis(rlmatrix):\n",
        "            I, J = calcuteIJ(rlmatrix)\n",
        "            numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, rlmatrix, (J*J)), axes=(0, 1))[0, 0]\n",
        "            S = calcuteS(rlmatrix)\n",
        "            return numerator / S\n",
        "    #2.LRE\n",
        "def getLongRunEmphasis(rlmatrix):\n",
        "        I, J = calcuteIJ(rlmatrix)\n",
        "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.multiply, rlmatrix, (J*J)), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        "    #3.GLN\n",
        "def getGrayLevelNonUniformity(rlmatrix):\n",
        "        G = np.apply_over_axes(np.sum, rlmatrix, axes=1)\n",
        "        numerator = np.apply_over_axes(np.sum, (G*G), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        "    # 4. RLN\n",
        "def getRunLengthNonUniformity(rlmatrix):\n",
        "            R = np.apply_over_axes(np.sum, rlmatrix, axes=0)\n",
        "            numerator = np.apply_over_axes(np.sum, (R*R), axes=(0, 1))[0, 0]\n",
        "            S = calcuteS(rlmatrix)\n",
        "            return numerator / S\n",
        "\n",
        "        # 5. RP\n",
        "def getRunPercentage(rlmatrix):\n",
        "            gray_level, run_length,_ = rlmatrix.shape\n",
        "            num_voxels = gray_level * run_length\n",
        "            return calcuteS(rlmatrix) / num_voxels\n",
        "\n",
        "        # 6. LGLRE\n",
        "def getLowGrayLevelRunEmphasis(rlmatrix):\n",
        "            I, J = calcuteIJ(rlmatrix)\n",
        "            numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, rlmatrix, (I*I)), axes=(0, 1))[0, 0]\n",
        "            S = calcuteS(rlmatrix)\n",
        "            return numerator / S\n",
        "\n",
        "        # 7. HGL   \n",
        "def getHighGrayLevelRunEmphais(rlmatrix):\n",
        "        I, J = calcuteIJ(rlmatrix)\n",
        "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.multiply, rlmatrix, (I*I)), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        "\n",
        "        # 8. SRLGLE\n",
        "def getShortRunLowGrayLevelEmphasis(rlmatrix):\n",
        "        I, J = calcuteIJ(rlmatrix)\n",
        "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, rlmatrix, (I*I*J*J)), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        "    # 9. SRHGLE\n",
        "def getShortRunHighGrayLevelEmphasis(rlmatrix):\n",
        "        I, J = calcuteIJ(rlmatrix)\n",
        "        temp = apply_over_degree(np.multiply, rlmatrix, (I*I))\n",
        "        print('-----------------------')\n",
        "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, temp, (J*J)), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        " \n",
        "    # 10. LRLGLE\n",
        "def getLongRunLow(rlmatrix):\n",
        "        I, J = calcuteIJ(rlmatrix)\n",
        "        temp = apply_over_degree(np.multiply, rlmatrix, (J*J))\n",
        "        numerator = np.apply_over_axes(np.sum, apply_over_degree(np.divide, temp, (J*J)), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        " \n",
        "    # 11. LRHGLE\n",
        "def getLongRunHighGrayLevelEmphais(rlmatrix):\n",
        "        I, J = calcuteIJ(rlmatrix)\n",
        "        numerator = np.apply_over_axes(np.sum,apply_over_degree(np.multiply, rlmatrix, (I*I*J*J)), axes=(0, 1))[0, 0]\n",
        "        S = calcuteS(rlmatrix)\n",
        "        return numerator / S\n",
        "\n",
        "#import getGrayRumatrix\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "import csv\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "tic=time.time()\n",
        "\n",
        "img_dir='/content/sample_data' #Enter the directory where all the images are stored\n",
        "data_path=os.path.join(img_dir,'*g')\n",
        "files=glob.glob(data_path)\n",
        "i=0\n",
        "for path in files:\n",
        "        data = 0\n",
        "        img = Image.open(path) \n",
        "        img = img.convert('L')\n",
        "        data=np.array(img)\n",
        "\n",
        "        DEG = [['deg0'], ['deg45'], ['deg90'], ['deg135']]\n",
        "\n",
        "        with open('GLRLM.csv','a+',newline='',encoding='utf-8') as f:\n",
        "                csv_writer = csv.writer(f)\n",
        "                \n",
        "                if i==0:\n",
        "                        csv_writer.writerow(['deg0_SRE','deg45_SRE','deg90_SRE','deg135_SRE','deg0_LRE','deg45_LRE','deg90_LRE','deg135_LRE',\n",
        "                                             'deg0_GLN','deg45_GLN','deg90_GLN','deg135_GLN','deg0_RLN','deg45_RLN','deg90_RLN','deg135_RLN',\n",
        "                                             'deg0_RP','deg45_RP','deg90_RP','deg135_RP','deg0_LGLRE','deg45_LGLRE','deg90_LGLRE','deg135_LGLRE',\n",
        "                                             'deg0_HGL','deg45_HGL','deg90_HGL','deg135_HGL','deg0_SRLGLE','deg45_SRLGLE','deg90_SRLGLE','deg135_SRLGLE',\n",
        "                                             'deg0_SRHGLE','deg45_SRHGLE','deg90_SRHGLE','deg135_SRHGLE','deg0_LRLGLE','deg45_LRLGLE','deg90_LRLGLE','deg135_LRLGLE',\n",
        "                                             'deg0_LRHGLE','deg45_LRHGLE','deg90_LRHGLE','deg135_LRHGLE'])\n",
        "                \n",
        "                print(\"Processing Image\",i+1)\n",
        "                i+=1\n",
        "                SRE_l=[]\n",
        "                LRE_l=[]\n",
        "                GLN_l=[]\n",
        "                RLN_l=[]\n",
        "                RP_l=[]\n",
        "                LGLRE_l=[]\n",
        "                HGL_l=[]\n",
        "                SRLGLE_l=[]\n",
        "                SRHGLE_l=[]\n",
        "                LRLGLE_l=[]\n",
        "                LRHGLE_l=[]\n",
        "                for deg in DEG:\n",
        "                    now_deg = deg[0]\n",
        "                    test_data = getGrayLevelRumatrix(data,deg)\n",
        "                    \n",
        "\n",
        "                    #1\n",
        "                    SRE = getShortRunEmphasis(test_data) \n",
        "                    SRE = np.squeeze(SRE)\n",
        "                    SRE_l.append(SRE)\n",
        "                 \n",
        "\n",
        "                    #2\n",
        "                    LRE = getLongRunEmphasis(test_data)\n",
        "                    LRE = np.squeeze(LRE)\n",
        "                    LRE_l.append(LRE)\n",
        "                    \n",
        "                    #3\n",
        "                    GLN = getGrayLevelNonUniformity(test_data)\n",
        "                    GLN = np.squeeze(GLN)\n",
        "                    GLN_l.append(GLN)\n",
        "                    \n",
        "                    #4\n",
        "                    RLN = getRunLengthNonUniformity(test_data)\n",
        "                    RLN = np.squeeze(RLN)\n",
        "                    RLN_l.append(RLN)\n",
        "                    \n",
        "                    #5\n",
        "                    RP = getRunPercentage(test_data)\n",
        "                    RP = np.squeeze(RP)\n",
        "                    RP_l.append(RP)\n",
        "                    \n",
        "                    #6\n",
        "                    LGLRE = getLowGrayLevelRunEmphasis(test_data)\n",
        "                    LGLRE = np.squeeze(LGLRE)\n",
        "                    LGLRE_l.append(LGLRE)\n",
        "                    \n",
        "                    #7\n",
        "                    HGL = getHighGrayLevelRunEmphais(test_data)\n",
        "                    HGL = np.squeeze(HGL)\n",
        "                    HGL_l.append(HGL)\n",
        "                    \n",
        "                    #8\n",
        "                    SRLGLE = getShortRunLowGrayLevelEmphasis(test_data)\n",
        "                    SRLGLE = np.squeeze(SRLGLE)\n",
        "                    SRLGLE_l.append(SRLGLE)\n",
        "                    \n",
        "                    #9\n",
        "                    SRHGLE = getShortRunHighGrayLevelEmphasis(test_data)\n",
        "                    SRHGLE = np.squeeze(SRHGLE)\n",
        "                    SRHGLE_l.append(SRHGLE)\n",
        "                    \n",
        "                    #10\n",
        "                    LRLGLE = getLongRunLow(test_data)\n",
        "                    LRLGLE = np.squeeze(LRLGLE)\n",
        "                    LRLGLE_l.append(LRLGLE)\n",
        "                    \n",
        "                    #11\n",
        "                    LRHGLE = getLongRunHighGrayLevelEmphais(test_data)\n",
        "                    LRHGLE = np.squeeze(LRHGLE)\n",
        "                    LRHGLE_l.append(LRHGLE)\n",
        "            \n",
        "\n",
        "                csv_writer.writerow(SRE_l+LRE_l+GLN_l+RLN_l+RP_l+LGLRE_l+HGL_l+SRLGLE_l+SRHGLE_l+LRLGLE_l+LRHGLE_l)\n",
        "\n",
        "toc=time.time()\n",
        "print(\"Computation time is: {} minutes.\".format(str((toc-tic)/60)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gBvHqt12ZzJ",
        "outputId": "fbe23138-1037-45db-8e3e-38ea7522082b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computation time is: 3.362099329630534e-05 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAY LEVEL CO-OCCURENCE MATRIX"
      ],
      "metadata": {
        "id": "ErwoSRKp2u4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GLCM or Gray Level Co-occurence Matrix extracts texture features from images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from skimage import data\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from skimage.transform import resize\n",
        "import time\n",
        "\n",
        "tic=time.time()\n",
        "\n",
        "PATCH_SIZE = 21\n",
        "\n",
        "#GLCM will work on batch of images only if all the images are of same size. \n",
        "#Uncomment the following two lines of code and enter the dimensions of images you want if the dataset has inconsistent sizes of images:\n",
        "\n",
        "#IMAGE_HEIGHT=\n",
        "#IMAGE_WIDTH=\n",
        "import os\n",
        "os.chdir('/content/sample_data/Tongue')\n",
        "img_dir = '/content/sample_data/Tongue/0CA41A93-47F3-40CD-9479-8FB4CCFED190 - vinay kumar.jpeg' #Enter the directory where all the images are stored\n",
        "data_path=os.path.join(img_dir,'*g')\n",
        "files=glob.glob(data_path)\n",
        "\n",
        "eo=len(files)\n",
        "\n",
        "img = []\n",
        "for f1 in files:\n",
        "    data = cv2.imread(f1)\n",
        "    img.append(data)\n",
        "\n",
        "for i in range(eo):\n",
        "        img[i] = cv2.cvtColor(img[i] , cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        #Uncomment the following line for inconsistent size of images in dataset\n",
        "        #img[i] = cv2.resize(img[i],(IMAGE_WIDTH,IMAGE_HEIGHT),interpolation=cv2.INTER_AREA)\n",
        "        \n",
        "        print(\"For image number:\"+str(i+1)+'\\n')\n",
        "        image=img[i]\n",
        "        print(\"Shape of image is: \",image.shape)\n",
        "        \n",
        "        # select some patches from grassy areas of the image\n",
        "        grass_locations = [(1000,100), (980,100), (990,120), (985,150)]\n",
        "        grass_patches = []\n",
        "        for loc in grass_locations:\n",
        "            grass_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,loc[1]:loc[1] + PATCH_SIZE])\n",
        "\n",
        "        # select some patches from sky areas of the image\n",
        "        sky_locations = [(417, 415), (427, 413), (420, 410), (422, 412)]\n",
        "        sky_patches = []\n",
        "        for loc in sky_locations:\n",
        "            sky_patches.append(image[loc[0]:loc[0] + PATCH_SIZE, loc[1]:loc[1] + PATCH_SIZE])\n",
        "\n",
        "        # compute some GLCM properties each patch\n",
        "        xs = []\n",
        "        ys = []\n",
        "        bs = []\n",
        "        cs = []\n",
        "        ds = []\n",
        "\n",
        "        for patch in (grass_patches + sky_patches):\n",
        "            glcm = greycomatrix(patch, distances=[5], angles=[0], levels=256,symmetric=True, normed=True)\n",
        "            xs.append(greycoprops(glcm, 'dissimilarity')[0, 0])\n",
        "            ys.append(greycoprops(glcm, 'correlation')[0, 0])\n",
        "            bs.append(greycoprops(glcm, 'contrast')[0, 0])\n",
        "            cs.append(greycoprops(glcm, 'energy')[0, 0])\n",
        "            ds.append(greycoprops(glcm, 'homogeneity')[0, 0])\n",
        "\n",
        "        temp_xs=xs\n",
        "        temp_ys=ys\n",
        "        temp_bs=bs\n",
        "        temp_cs=cs\n",
        "        temp_ds=ds\n",
        "\n",
        "        temp_xs.sort()\n",
        "        temp_ys.sort()\n",
        "        temp_bs.sort()\n",
        "        temp_cs.sort()\n",
        "        temp_ds.sort()\n",
        "\n",
        "        xs_max=temp_xs[-1]\n",
        "        ys_max=temp_ys[-1]\n",
        "        bs_max=temp_bs[-1]\n",
        "        cs_max=temp_cs[-1]\n",
        "        ds_max=temp_ds[-1]\n",
        "\n",
        "        xs_mean=sum(temp_xs)/len(xs)\n",
        "        ys_mean=sum(temp_ys)/len(ys)\n",
        "        bs_mean=sum(temp_bs)/len(bs)\n",
        "        cs_mean=sum(temp_cs)/len(cs)\n",
        "        ds_mean=sum(temp_ds)/len(ds)\n",
        "\n",
        "        xs_var=np.var(np.array(temp_xs))\n",
        "        ys_var=np.var(np.array(temp_ys))\n",
        "        bs_var=np.var(np.array(temp_bs))\n",
        "        cs_var=np.var(np.array(temp_cs))\n",
        "        ds_var=np.var(np.array(temp_ds))\n",
        "\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "        df['dissimilarity_max'] = [xs_max]\n",
        "        df['dissimilarity_mean'] = xs_mean\n",
        "        df['dissimilarity_var'] = xs_var\n",
        "\n",
        "        df['correlation_max'] = ys_max\n",
        "        df['correlation_mean'] = ys_mean\n",
        "        df['correlation_var'] = ys_var\n",
        "\n",
        "        df['contrast_max'] = bs_max\n",
        "        df['contrast_mean'] = bs_mean\n",
        "        df['contrast_var'] = bs_var\n",
        "\n",
        "        df['energy_max'] = cs_max\n",
        "        df['energy_mean'] = cs_mean\n",
        "        df['energy_var'] = cs_var\n",
        "\n",
        "        df['homogeneity_max'] = ds_max\n",
        "        df['homogeneity_mean'] = ds_mean\n",
        "        df['homogeneity_var'] = ds_var\n",
        "\n",
        "        print(df)\n",
        "        \n",
        "        with open('GLCM_BreCaHAD_temp.csv','a+',newline='') as file:\n",
        "            writer=csv.writer(file)\n",
        "            \n",
        "            if i==0:\n",
        "                writer.writerow(['dissimilarity_max','dissimilarity_mean','dissimilarity_var','contrast_max','contrast_mean','contrast_var','energy_max','energy_mean',\n",
        "                                 'energy_var','correlation_max','correlation_mean','correlation_var','homogeneity_max','homogeneity_mean','homogeneity_var'])\n",
        "            \n",
        "            writer.writerow([xs_max,xs_mean,xs_var,ys_max,ys_mean,ys_var,bs_max,bs_mean,bs_var,cs_max,cs_mean,cs_var,ds_max,ds_mean,ds_var])\n",
        "\n",
        "#Uncomment the following section for features visualization\n",
        "        \n",
        "        #create the figure\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        # display original image with locations of patches\n",
        "        ax = fig.add_subplot(3, 2, 1)\n",
        "        ax.imshow(image, cmap=plt.cm.gray,\n",
        "                  vmin=0, vmax=255)\n",
        "        for (y, x) in grass_locations:\n",
        "            ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'gs')\n",
        "        for (y, x) in sky_locations:\n",
        "            ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'bs')\n",
        "        ax.set_xlabel('Original Image')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.axis('image')\n",
        "        # for each patch, plot (dissimilarity, correlation)\n",
        "        ax = fig.add_subplot(3, 2, 2)\n",
        "        ax.plot(xs[:len(grass_patches)], ys[:len(grass_patches)], 'go',\n",
        "                label='Grass')\n",
        "        ax.plot(xs[len(grass_patches):], ys[len(grass_patches):], 'bo',\n",
        "                label='Sky')\n",
        "        ax.set_xlabel('GLCM Dissimilarity')\n",
        "        ax.set_ylabel('GLCM Correlation')\n",
        "        ax.legend()\n",
        "        # display the image patches\n",
        "        for i, patch in enumerate(grass_patches):\n",
        "            ax = fig.add_subplot(3, len(grass_patches), len(grass_patches)*1 + i + 1)\n",
        "            ax.imshow(patch, cmap=plt.cm.gray,\n",
        "                      vmin=0, vmax=255)\n",
        "            ax.set_xlabel('Grass %d' % (i + 1))\n",
        "        for i, patch in enumerate(sky_patches):\n",
        "            ax = fig.add_subplot(3, len(sky_patches), len(sky_patches)*2 + i + 1)\n",
        "            ax.imshow(patch, cmap=plt.cm.gray,\n",
        "                      vmin=0, vmax=255)\n",
        "            ax.set_xlabel('Sky %d' % (i + 1))\n",
        "        # display the patches and plot\n",
        "        fig.suptitle('Grey level co-occurrence matrix features', fontsize=14, y=1.05)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "toc=time.time()\n",
        "print('Computation time is : {} seconds'.format(str(toc-tic)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djcFnmPz2x2o",
        "outputId": "68b02103-8bd5-4ea1-f285-7f8c9260a8d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computation time is : 0.0013070106506347656 seconds\n"
          ]
        }
      ]
    }
  ]
}